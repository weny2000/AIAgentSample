# Next.js -> Python service URL
STRANDS_SERVICE_URL=http://localhost:8001

# Which LLM provider to use: ollama | aws | openai_compat
STRANDS_LLM_PROVIDER=openai_compat

# Ollama settings
STRANDS_OLLAMA_BASE_URL=http://localhost:11434
STRANDS_OLLAMA_MODEL=gpt-oss:20b

# AWS Bedrock settings
STRANDS_AWS_REGION=us-west-2
STRANDS_AWS_MODEL=us.amazon.nova-pro-v1:0

# OpenAI-compatible settings
STRANDS_OPENAI_BASE_URL=http://localhost:8899/v1
STRANDS_OPENAI_MODEL=gpt-5
STRANDS_OPENAI_API_KEY=aaa
